{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from re import I\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "torch.cuda.set_device(2)\n",
    "device = 'cuda:2'\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../video_diffusion_pytorch/')\n",
    "from vq_gan_3d.model.vqgan import VQGAN\n",
    "from vq_gan_3d.dataset import MRNetDataset, BRATSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n",
    "import pytorch_ssim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = BRATSDataset(root_dir='/data/BraTS/BraTS 2020', train=False, imgtype='flair')\n",
    "\n",
    "trainset = BRATSDataset(root_dir='/data/BraTS/BraTS 2020', train=True, imgtype='flair')\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size = 2, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from /data/home/firas/Desktop/work/other_groups/vq_gan_3d/model/cache/vgg.pth\n",
      "found 118 videos as gif files at None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vqgan_ckpt = '/data/home/firas/Desktop/work/other_groups/video_diffusion_pytorch/vqgan_checkpoints/brats/epoch=728-step=153000-train/recon_loss=0.06.ckpt'\n",
    "\n",
    "model = Unet3D(\n",
    "    dim=32,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    channels=8,\n",
    ").cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    vqgan_ckpt=vqgan_ckpt,\n",
    "    image_size=32,\n",
    "    num_frames=32,\n",
    "    channels=8,\n",
    "    timesteps=300,           # number of steps\n",
    "    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    "    # sampling_timesteps=250,\n",
    "    loss_type='l1',            # L1 or L2\n",
    "    # objective=args.objective\n",
    ").cuda()\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    args=None,\n",
    "    dataset=val_dataset,\n",
    "    train_batch_size=1,\n",
    "    save_and_sample_every=10,\n",
    "    train_num_steps=700000,         # total training steps\n",
    "    gradient_accumulate_every=2,    # gradient accumulation steps\n",
    "    ema_decay=0.995,                # exponential moving average decay\n",
    "    amp=False,                       # turn on mixed precision\n",
    "    num_sample_rows=1\n",
    "    # logger=logger\n",
    ")\n",
    "\n",
    "trainer.load('/data/home/firas/Desktop/work/other_groups/video_diffusion_pytorch/results/model-82.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7385)\n",
      "tensor(0.7440)\n",
      "tensor(0.7443)\n"
     ]
    }
   ],
   "source": [
    "sum_ssim = 0\n",
    "for k in range(20):\n",
    "    for i,dat in enumerate(train_loader):\n",
    "        dat = dat['data']\n",
    "        if len(dat)!=2:\n",
    "            print(\"Length: \", len(dat))\n",
    "            break\n",
    "        img1 = dat[0]\n",
    "        img2 = dat[1]\n",
    "\n",
    "        msssim = pytorch_ssim.msssim_3d(img1,img2)\n",
    "        sum_ssim = sum_ssim+msssim\n",
    "    print(sum_ssim/((k+1)*(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 300/300 [00:18<00:00, 16.13it/s]\n",
      "sampling loop time step:  99%|█████████▉| 297/300 [00:18<00:00, 15.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39mema_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m     sample \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mema_model\u001b[39m.\u001b[39;49msample(batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m img1 \u001b[39m=\u001b[39m sample[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/test.ipynb#ch0000007vscode-remote?line=6'>7</a>\u001b[0m img2 \u001b[39m=\u001b[39m sample[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:768\u001b[0m, in \u001b[0;36mGaussianDiffusion.sample\u001b[0;34m(self, cond, cond_scale, batch_size)\u001b[0m\n\u001b[1;32m    766\u001b[0m channels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels\n\u001b[1;32m    767\u001b[0m num_frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_frames\n\u001b[0;32m--> 768\u001b[0m _sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m    769\u001b[0m     (batch_size, channels, num_frames, image_size, image_size), cond\u001b[39m=\u001b[39;49mcond, cond_scale\u001b[39m=\u001b[39;49mcond_scale)\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvqgan, VQGAN):\n\u001b[1;32m    772\u001b[0m     \u001b[39m# denormalize TODO: Remove eventually\u001b[39;00m\n\u001b[1;32m    773\u001b[0m     _sample \u001b[39m=\u001b[39m (((_sample \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvqgan\u001b[39m.\u001b[39mcodebook\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m\n\u001b[1;32m    774\u001b[0m                                           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvqgan\u001b[39m.\u001b[39mcodebook\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mmin())) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvqgan\u001b[39m.\u001b[39mcodebook\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mmin()\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:752\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[0;34m(self, shape, cond, cond_scale)\u001b[0m\n\u001b[1;32m    749\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(shape, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    751\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msampling loop time step\u001b[39m\u001b[39m'\u001b[39m, total\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps):\n\u001b[0;32m--> 752\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(img, torch\u001b[39m.\u001b[39;49mfull(\n\u001b[1;32m    753\u001b[0m         (b,), i, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong), cond\u001b[39m=\u001b[39;49mcond, cond_scale\u001b[39m=\u001b[39;49mcond_scale)\n\u001b[1;32m    755\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:736\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, x, t, cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39minference_mode()\n\u001b[1;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_sample\u001b[39m(\u001b[39mself\u001b[39m, x, t, cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cond_scale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, clip_denoised\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    735\u001b[0m     b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m--> 736\u001b[0m     model_mean, _, model_log_variance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[1;32m    737\u001b[0m         x\u001b[39m=\u001b[39;49mx, t\u001b[39m=\u001b[39;49mt, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, cond\u001b[39m=\u001b[39;49mcond, cond_scale\u001b[39m=\u001b[39;49mcond_scale)\n\u001b[1;32m    738\u001b[0m     noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m    739\u001b[0m     \u001b[39m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:712\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, x, t, clip_denoised, cond, cond_scale)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\u001b[39mself\u001b[39m, x, t, clip_denoised: \u001b[39mbool\u001b[39m, cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cond_scale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m):\n\u001b[1;32m    711\u001b[0m     x_recon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_start_from_noise(\n\u001b[0;32m--> 712\u001b[0m         x, t\u001b[39m=\u001b[39mt, noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdenoise_fn\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, t, cond\u001b[39m=\u001b[39;49mcond, cond_scale\u001b[39m=\u001b[39;49mcond_scale))\n\u001b[1;32m    714\u001b[0m     \u001b[39mif\u001b[39;00m clip_denoised:\n\u001b[1;32m    715\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:508\u001b[0m, in \u001b[0;36mUnet3D.forward_with_cond_scale\u001b[0;34m(self, cond_scale, *args, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_with_cond_scale\u001b[39m(\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    504\u001b[0m     \u001b[39m*\u001b[39margs,\n\u001b[1;32m    505\u001b[0m     cond_scale\u001b[39m=\u001b[39m\u001b[39m2.\u001b[39m,\n\u001b[1;32m    506\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    507\u001b[0m ):\n\u001b[0;32m--> 508\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, null_cond_prob\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    509\u001b[0m     \u001b[39mif\u001b[39;00m cond_scale \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_cond:\n\u001b[1;32m    510\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:555\u001b[0m, in \u001b[0;36mUnet3D.forward\u001b[0;34m(self, x, time, cond, null_cond_prob, focus_present_mask, prob_focus_present)\u001b[0m\n\u001b[1;32m    553\u001b[0m x \u001b[39m=\u001b[39m block1(x, t)\n\u001b[1;32m    554\u001b[0m x \u001b[39m=\u001b[39m block2(x, t)\n\u001b[0;32m--> 555\u001b[0m x \u001b[39m=\u001b[39m spatial_attn(x)\n\u001b[1;32m    556\u001b[0m x \u001b[39m=\u001b[39m temporal_attn(x, pos_bias\u001b[39m=\u001b[39mtime_rel_pos_bias,\n\u001b[1;32m    557\u001b[0m                   focus_present_mask\u001b[39m=\u001b[39mfocus_present_mask)\n\u001b[1;32m    558\u001b[0m h\u001b[39m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:148\u001b[0m, in \u001b[0;36mResidual.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39m+\u001b[39m x\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:194\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    193\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/home/firas/Desktop/work/other_groups/vq_gan_3d/evaluation/../../video_diffusion_pytorch/video_diffusion_pytorch/video_diffusion_pytorch.py:259\u001b[0m, in \u001b[0;36mSpatialLinearAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m x \u001b[39m=\u001b[39m rearrange(x, \u001b[39m'\u001b[39m\u001b[39mb c f h w -> (b f) c h w\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    258\u001b[0m qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_qkv(x)\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m q, k, v \u001b[39m=\u001b[39m rearrange_many(\n\u001b[1;32m    260\u001b[0m     qkv, \u001b[39m'\u001b[39m\u001b[39mb (h c) x y -> b h c (x y)\u001b[39m\u001b[39m'\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)\n\u001b[1;32m    262\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    263\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/einops_exts/einops_exts.py:19\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(tensors, pattern, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m (fn(tensor, pattern, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/einops/einops.py:487\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRearrange can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be applied to an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m     tensor \u001b[39m=\u001b[39m get_backend(tensor[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[0;32m--> 487\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/einops/einops.py:410\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    408\u001b[0m     hashable_axes_lengths \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(axes_lengths\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    409\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(recipe, tensor, reduction_type\u001b[39m=\u001b[39;49mreduction)\n\u001b[1;32m    411\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    412\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Error while processing \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-reduction pattern \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/einops/einops.py:236\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m    234\u001b[0m tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n\u001b[1;32m    235\u001b[0m tensor \u001b[39m=\u001b[39m _reduce_axes(tensor, reduction_type\u001b[39m=\u001b[39mreduction_type, reduced_axes\u001b[39m=\u001b[39mreduced_axes, backend\u001b[39m=\u001b[39mbackend)\n\u001b[0;32m--> 236\u001b[0m tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mtranspose(tensor, axes_reordering)\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(added_axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    238\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39madd_axes(tensor, n_axes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(axes_reordering) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(added_axes), pos2len\u001b[39m=\u001b[39madded_axes)\n",
      "File \u001b[0;32m/data/home/firas/anaconda3/envs/vq_gan_3d/lib/python3.8/site-packages/einops/_backends.py:331\u001b[0m, in \u001b[0;36mTorchBackend.transpose\u001b[0;34m(self, x, axes)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose\u001b[39m(\u001b[39mself\u001b[39m, x, axes):\n\u001b[0;32m--> 331\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mpermute(axes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sum_ssim = 0\n",
    "for i in range(1000):\n",
    "    trainer.ema_model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = trainer.ema_model.sample(batch_size=2)\n",
    "    img1 = sample[0].cpu()\n",
    "    img2 = sample[1].cpu()\n",
    "\n",
    "    msssim = pytorch_ssim.msssim_3d(img1,img2)\n",
    "    sum_ssim = sum_ssim+msssim\n",
    "    print(sum_ssim/(i+1.0))\n",
    "print(sum_ssim/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vq_gan_3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7478f85da7647e86179867db06708d1793afbb0c5e99aae581399d853bf0a10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
